{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfd9558-2c2c-469c-adff-f430af8786eb",
   "metadata": {},
   "source": [
    "# Historical Full Basins: Extract Other Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f7894-71e2-49f0-9c52-958e28f81a25",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to extract the annual temperature time series in the format needed for weather generator formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1b2d8-d6a2-48e6-a4b7-eff65810e619",
   "metadata": {},
   "source": [
    "## Imports and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbb4997-fba1-4e18-89a7-1cde9c8fa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ad22b6-be58-41fc-8b46-9d37f9581bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "import datetime as dt\n",
    "from IPython.display import display, HTML, Image\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56abefed-a88b-412b-84b4-79a6db07b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StartTS = pd.Timestamp( 1991, 1, 1, )\n",
    "EndTS = pd.Timestamp(2020, 12, 31, 23, 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8d4b31-6fa5-4b4c-8d8c-840347acf016",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR1 = r'C:\\Users\\nmartin\\Documents\\EAA_HCP\\Data\\SwRI_Processed\\Processed_Historical'\n",
    "OUT_DIR = r'C:\\Users\\nmartin\\Documents\\EAA_HCP\\Data\\SwRI_Processed\\Processed_Historical\\OtherWeather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ff7b11-7333-4980-b2d0-81b9f251699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CutThresh = 0.255\n",
    "TOTYRDAYS = 366"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f8ef1-75ad-4cf8-8936-7492c7220dfb",
   "metadata": {},
   "source": [
    "## Load Dictionary of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125773a4-4f66-4415-aca9-5f2b97297af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "InFiler = os.path.normpath( os.path.join( IN_DIR1, \"FBas_MetTS_1980through2020_Dict.pkl\" ) )\n",
    "with open( InFiler, 'rb') as IF:\n",
    "    BasinsDFDict = pickle.load(IF)\n",
    "# end with\n",
    "basKeys = sorted( BasinsDFDict.keys() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311daa0f-5f84-4ef0-b027-9cb8d949d193",
   "metadata": {},
   "source": [
    "## Get the Averages DataFrames by Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c25786e-a4c5-4780-84ee-16f0d9149b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1991, 2020)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StartYr = StartTS.year\n",
    "StopYr = EndTS.year\n",
    "StartYr, StopYr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af525f4-282e-4196-b7ae-c3f651422706",
   "metadata": {},
   "outputs": [],
   "source": [
    "OWDryAvesDict = dict()\n",
    "OWWetAvesDict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46cf3eae-750b-44ba-9405-576413752682",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bKey in basKeys:\n",
    "    cBasDF = BasinsDFDict[bKey]\n",
    "    WStateCnt = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "    tWMax = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tWAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tWMin = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    DStateCnt = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "    tDMax = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tDAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tDMin = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    for yY in range(StartYr, StopYr+1):\n",
    "        curStart = pd.Timestamp( yY, 1, 1, 0, )\n",
    "        curStop = pd.Timestamp( yY, 12, 31, 23, 59, )\n",
    "        curYrDF = cBasDF.loc[curStart:curStop].copy()\n",
    "        NumDays = len( curYrDF )\n",
    "        if NumDays > 365:\n",
    "            LastDay = TOTYRDAYS\n",
    "        else:\n",
    "            LastDay = ( TOTYRDAYS - 1 )\n",
    "        # end if\n",
    "        IsWet = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsDry = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsWet[:LastDay] = np.where( curYrDF[\"Pre_mm\"] >= CutThresh, 1, 0 )\n",
    "        IsDry[:LastDay] = np.where( curYrDF[\"Pre_mm\"] < CutThresh, 1, 0 )\n",
    "        WStateCnt[:LastDay] += IsWet[:LastDay]\n",
    "        DStateCnt[:LastDay] += IsDry[:LastDay]\n",
    "        tWMax[:LastDay] += ( curYrDF[\"MaxT_C\"].to_numpy(dtype=np.float32) * IsWet[:LastDay] )\n",
    "        tDMax[:LastDay] += ( curYrDF[\"MaxT_C\"].to_numpy(dtype=np.float32) * IsDry[:LastDay] )\n",
    "        tWAve[:LastDay] += ( curYrDF[\"AveT_C\"].to_numpy(dtype=np.float32) * IsWet[:LastDay] )\n",
    "        tDAve[:LastDay] += ( curYrDF[\"AveT_C\"].to_numpy(dtype=np.float32) * IsDry[:LastDay] )\n",
    "        tWMin[:LastDay] += ( curYrDF[\"MinT_C\"].to_numpy(dtype=np.float32) * IsWet[:LastDay] )\n",
    "        tDMin[:LastDay] += ( curYrDF[\"MinT_C\"].to_numpy(dtype=np.float32) * IsDry[:LastDay] )\n",
    "    # end for\n",
    "    wDenom = np.where( WStateCnt > 0, np.array( WStateCnt, dtype=np.float32), np.nan )\n",
    "    wMulti = 1.0 / wDenom\n",
    "    dDenom = np.where( DStateCnt > 0, np.array( DStateCnt, dtype=np.float32),np.nan )\n",
    "    dMulti = 1.0 / dDenom\n",
    "    WDDict = { \"Wet Counts\" : WStateCnt,\n",
    "               \"AMaxT_C\" : (tWMax * wMulti),\n",
    "               \"AAveT_C\" : (tWAve * wMulti),\n",
    "               \"AMinT_C\" : (tWMin * wMulti), }\n",
    "    DDDict = { \"Dry Counts\" : DStateCnt,\n",
    "               \"AMaxT_C\" : (tDMax * dMulti),\n",
    "               \"AAveT_C\" : (tDAve * dMulti),\n",
    "               \"AMinT_C\" : (tDMin * dMulti), }\n",
    "    # build our data frames\n",
    "    DaysIndexer = [ x for x in range(1, (TOTYRDAYS + 1), 1)]\n",
    "    WetDF = pd.DataFrame( index=DaysIndexer, data=WDDict )\n",
    "    DryDF = pd.DataFrame( index=DaysIndexer, data=DDDict )\n",
    "    OWDryAvesDict[bKey] = DryDF\n",
    "    OWWetAvesDict[bKey] = WetDF\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8a0ec-9bc3-44d2-8af5-a63db19ce7a4",
   "metadata": {},
   "source": [
    "Serialize these dictionaries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1315e9c-4e06-4309-a9f6-b6cebd9214e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeathWet_Ave_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( OWWetAvesDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57389fce-aabb-4de2-8961-71d21a0ade46",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeathDry_Ave_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( OWDryAvesDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74033e21-3f23-4993-9ad9-92a766b8d777",
   "metadata": {},
   "source": [
    "## Calculate Standard Deviation DataFrames by Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83001d0-2090-4378-a1b5-41881f65470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OWDryStdDict = dict()\n",
    "OWWetStdDict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5f7a8a-276c-4dec-a7fe-66d55c1030b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bKey in basKeys:\n",
    "    cBasDF = BasinsDFDict[bKey]\n",
    "    cBasWetAveDF = OWWetAvesDict[bKey]\n",
    "    cBasDryAveDF = OWDryAvesDict[bKey]\n",
    "    WStateCnt = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "    tWMax = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tWAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tWMin = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    DStateCnt = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "    tDMax = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tDAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    tDMin = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    curWAves = cBasWetAveDF.copy()\n",
    "    curDAves = cBasDryAveDF.copy()\n",
    "    for yY in range(StartYr, StopYr+1):\n",
    "        curStart = pd.Timestamp( yY, 1, 1, 0, )\n",
    "        curStop = pd.Timestamp( yY, 12, 31, 23, 59, )\n",
    "        curYrDF = cBasDF.loc[curStart:curStop].copy()\n",
    "        NumDays = len( curYrDF )\n",
    "        if NumDays > 365:\n",
    "            LastDay = TOTYRDAYS\n",
    "        else:\n",
    "            LastDay = ( TOTYRDAYS - 1 )\n",
    "        # end if\n",
    "        IsWet = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsDry = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsWet[:LastDay] = np.where( curYrDF[\"Pre_mm\"] >= CutThresh, 1, 0 )\n",
    "        IsDry[:LastDay] = np.where( curYrDF[\"Pre_mm\"] < CutThresh, 1, 0 )\n",
    "        WStateCnt[:LastDay] += IsWet[:LastDay]\n",
    "        DStateCnt[:LastDay] += IsDry[:LastDay]\n",
    "        # now do the standard deviation calculations\n",
    "        curWAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        # do max t\n",
    "        curWAve[:LastDay] = curWAves[\"AMaxT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves[\"AMaxT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        ctNew = curYrDF[\"MaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        tWMax[:LastDay] += ( IsWet[:LastDay] * ( ctNew - curWAve[:LastDay] )**2.0 )\n",
    "        tDMax[:LastDay] += ( IsDry[:LastDay] * ( ctNew - curDAve[:LastDay] )**2.0 )\n",
    "        # do ave T\n",
    "        curWAve[:LastDay] = curWAves[\"AAveT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves[\"AAveT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        ctNew = curYrDF[\"AveT_C\"].to_numpy(dtype=np.float32)\n",
    "        tWAve[:LastDay] += ( IsWet[:LastDay] * ( ctNew - curWAve[:LastDay] )**2.0 )\n",
    "        tDAve[:LastDay] += ( IsDry[:LastDay] * ( ctNew - curDAve[:LastDay] )**2.0 )\n",
    "         # do min t\n",
    "        curWAve[:LastDay] = curWAves[\"AMinT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves[\"AMinT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        ctNew = curYrDF[\"MinT_C\"].to_numpy(dtype=np.float32)\n",
    "        tWMin[:LastDay] += ( IsWet[:LastDay] * ( ctNew - curWAve[:LastDay] )**2.0 )\n",
    "        tDMin[:LastDay] += ( IsDry[:LastDay] * ( ctNew - curDAve[:LastDay] )**2.0 )\n",
    "    # end for\n",
    "    wDenom = np.where( WStateCnt > 0, np.array( WStateCnt, dtype=np.float32), np.nan )\n",
    "    wMulti = 1.0 / wDenom\n",
    "    dDenom = np.where( DStateCnt > 0, np.array( DStateCnt, dtype=np.float32),np.nan )\n",
    "    dMulti = 1.0 / dDenom\n",
    "    WDDict = { \"Wet Counts\" : WStateCnt,\n",
    "               \"SDMaxT_C\" : np.sqrt((tWMax * wMulti)),\n",
    "               \"SDAveT_C\" : np.sqrt((tWAve * wMulti)),\n",
    "               \"SDMinT_C\" : np.sqrt((tWMin * wMulti)), }\n",
    "    DDDict = { \"Dry Counts\" : DStateCnt,\n",
    "               \"SDMaxT_C\" : np.sqrt((tDMax * dMulti)),\n",
    "               \"SDAveT_C\" : np.sqrt((tDAve * dMulti)),\n",
    "               \"SDMinT_C\" : np.sqrt((tDMin * dMulti)), }\n",
    "    # build our data frames\n",
    "    DaysIndexer = [ x for x in range(1, (TOTYRDAYS + 1), 1)]\n",
    "    WetDF = pd.DataFrame( index=DaysIndexer, data=WDDict )\n",
    "    WetDF.interpolate( inplace=True )\n",
    "    DryDF = pd.DataFrame( index=DaysIndexer, data=DDDict )\n",
    "    DryDF.interpolate( inplace=True )\n",
    "    OWDryStdDict[bKey] = DryDF\n",
    "    OWWetStdDict[bKey] = WetDF\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f8ab0-72d2-4c0d-9814-4e0ce27ee51c",
   "metadata": {},
   "source": [
    "Serialize these dictionaries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd2762c9-c6a7-4e60-a184-85afde7a60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeathWet_Std_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( OWWetStdDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7ac4961-4d63-47c5-9277-63a63a9e01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeathDry_Std_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( OWDryStdDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517ef3f-7b87-46b3-a3fe-ca778f052ce9",
   "metadata": {},
   "source": [
    "## Calculate Z-Score by Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "377a9e01-1297-4671-8c16-f093b9256cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZTAveDict = dict()\n",
    "ZTMaxDict = dict()\n",
    "ZTMinDict = dict()\n",
    "TrMaxDict = dict()\n",
    "TrMinDict = dict()\n",
    "ZTRho0Dict = dict()\n",
    "ZTRho1Dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f22408-cf57-403a-8506-25bb98b5d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bKey in basKeys:\n",
    "    cBasDF = BasinsDFDict[bKey]\n",
    "    cBasWetAveDF = OWWetAvesDict[bKey]\n",
    "    cBasDryAveDF = OWDryAvesDict[bKey]\n",
    "    cBasWetStdDF = OWWetStdDict[bKey]\n",
    "    cBasDryStdDF = OWDryStdDict[bKey]\n",
    "    WStateCnt = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "    allZMax = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    allZAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    allZMin = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "    DStateCnt = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "    ZTMxMax = np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    ZTMxMin = 999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    TMxMax = -999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    TMxMin = 999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    ZTAvMax = np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    ZTAvMin = 999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    TAvMax = -999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    TAvMin = 999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    ZTMnMax = np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    ZTMnMin = 999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    TMnMax = -999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    TMnMin = 999.0 * np.ones( TOTYRDAYS, dtype=np.float32 )\n",
    "    curWAves = cBasWetAveDF.copy()\n",
    "    curDAves = cBasDryAveDF.copy()\n",
    "    curWStds = cBasWetStdDF.copy()\n",
    "    curDStds = cBasDryStdDF.copy()\n",
    "    for yY in range(StartYr, StopYr+1):\n",
    "        curStart = pd.Timestamp( yY, 1, 1, 0, )\n",
    "        curStop = pd.Timestamp( yY, 12, 31, 23, 59, )\n",
    "        curYrDF = cBasDF.loc[curStart:curStop].copy()\n",
    "        NumDays = len( curYrDF )\n",
    "        if NumDays > 365:\n",
    "            LastDay = TOTYRDAYS\n",
    "        else:\n",
    "            LastDay = ( TOTYRDAYS - 1 )\n",
    "        # end if\n",
    "        IsWet = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsDry = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsWet[:LastDay] = np.where( curYrDF[\"Pre_mm\"] >= CutThresh, 1, 0 )\n",
    "        IsDry[:LastDay] = np.where( curYrDF[\"Pre_mm\"] < CutThresh, 1, 0 )\n",
    "        WStateCnt[:LastDay] += IsWet[:LastDay]\n",
    "        DStateCnt[:LastDay] += IsDry[:LastDay]\n",
    "        # now calculate the Zs\n",
    "        curWAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curWStd = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDStd = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        # do max t\n",
    "        curWAve[:LastDay] = curWAves[\"AMaxT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves[\"AMaxT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curWStd[:LastDay] = curWStds[\"SDMaxT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDStd[:LastDay] = curDStds[\"SDMaxT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curWStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curWStd, out=curWStdM, where=curWStd>0.01 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curDStd, out=curDStdM, where=curDStd>0.01 )\n",
    "        ctNew = curYrDF[\"MaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        wZs = ( IsWet[:LastDay] * ( ( ctNew - curWAve[:LastDay] ) * curWStdM[:LastDay] ) )\n",
    "        dZs = ( IsDry[:LastDay] * ( ( ctNew - curDAve[:LastDay] ) * curDStdM[:LastDay] ) )\n",
    "        allZ = wZs + dZs\n",
    "        allZMax[:LastDay] += allZ\n",
    "        ZTMxMax[:LastDay] = np.where( allZ > ZTMxMax[:LastDay], allZ, ZTMxMax[:LastDay] )\n",
    "        ZTMxMin[:LastDay] = np.where( allZ < ZTMxMin[:LastDay], allZ, ZTMxMin[:LastDay] )\n",
    "        TMxMax[:LastDay] = np.where( ctNew > TMxMax[:LastDay], ctNew, TMxMax[:LastDay] )\n",
    "        TMxMin[:LastDay] = np.where( ctNew < TMxMin[:LastDay], ctNew, TMxMin[:LastDay] )\n",
    "        # do ave t\n",
    "        curWAve[:LastDay] = curWAves[\"AAveT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves[\"AAveT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curWStd[:LastDay] = curWStds[\"SDAveT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDStd[:LastDay] = curDStds[\"SDAveT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curWStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curWStd, out=curWStdM, where=curWStd>0.01 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curDStd, out=curDStdM, where=curDStd>0.01 )\n",
    "        ctNew = curYrDF[\"AveT_C\"].to_numpy(dtype=np.float32)\n",
    "        wZs = ( IsWet[:LastDay] * ( ( ctNew - curWAve[:LastDay] ) * curWStdM[:LastDay] ) )\n",
    "        dZs = ( IsDry[:LastDay] * ( ( ctNew - curDAve[:LastDay] ) * curDStdM[:LastDay] ) )\n",
    "        allZ = wZs + dZs\n",
    "        allZAve[:LastDay] += allZ\n",
    "        ZTAvMax[:LastDay] = np.where( allZ > ZTAvMax[:LastDay], allZ, ZTAvMax[:LastDay] )\n",
    "        ZTAvMin[:LastDay] = np.where( allZ < ZTAvMin[:LastDay], allZ, ZTAvMin[:LastDay] )\n",
    "        TAvMax[:LastDay] = np.where( ctNew > TAvMax[:LastDay], ctNew, TAvMax[:LastDay] )\n",
    "        TAvMin[:LastDay] = np.where( ctNew < TAvMin[:LastDay], ctNew, TAvMin[:LastDay] )\n",
    "        # do min t\n",
    "        curWAve[:LastDay] = curWAves[\"AMinT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves[\"AMinT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curWStd[:LastDay] = curWStds[\"SDMinT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curDStd[:LastDay] = curDStds[\"SDMinT_C\"].loc[:LastDay].to_numpy(dtype=np.float32)\n",
    "        curWStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curWStd, out=curWStdM, where=curWStd>0.01 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curDStd, out=curDStdM, where=curDStd>0.01 )\n",
    "        ctNew = curYrDF[\"MinT_C\"].to_numpy(dtype=np.float32)\n",
    "        wZs = ( IsWet[:LastDay] * ( ( ctNew - curWAve[:LastDay] ) * curWStdM[:LastDay] ) )\n",
    "        dZs = ( IsDry[:LastDay] * ( ( ctNew - curDAve[:LastDay] ) * curDStdM[:LastDay] ) )\n",
    "        allZ = wZs + dZs\n",
    "        allZMin[:LastDay] += allZ\n",
    "        ZTMnMax[:LastDay] = np.where( allZ > ZTMnMax[:LastDay], allZ, ZTMnMax[:LastDay] )\n",
    "        ZTMnMin[:LastDay] = np.where( allZ < ZTMnMin[:LastDay], allZ, ZTMnMin[:LastDay] )\n",
    "        TMnMax[:LastDay] = np.where( ctNew > TMnMax[:LastDay], ctNew, TMnMax[:LastDay] )\n",
    "        TMnMin[:LastDay] = np.where( ctNew < TMnMin[:LastDay], ctNew, TMnMin[:LastDay] )\n",
    "    # end for\n",
    "    AllStateCnt = WStateCnt + DStateCnt\n",
    "    ZMaxDict = { \"All Counts\" : AllStateCnt,\n",
    "                 \"MxZMaxT_C\" : ZTMxMax,\n",
    "                 \"MxZAveT_C\" : ZTAvMax,\n",
    "                 \"MxZMinT_C\" : ZTMnMax, }\n",
    "    MaxDict = { \"AllCounts\" : AllStateCnt,\n",
    "                \"MxMaxT_C\" : TMxMax,\n",
    "                \"MxAveT_C\" : TAvMax,\n",
    "                \"MxMinT_C\" : TMnMax, }\n",
    "    ZMinDict = { \"All Counts\" : AllStateCnt,\n",
    "                 \"MnZMaxT_C\" : ZTMxMin,\n",
    "                 \"MnZAveT_C\" : ZTAvMin,\n",
    "                 \"MnZMinT_C\" : ZTMnMin, }\n",
    "    MinDict = { \"AllCounts\" : AllStateCnt,\n",
    "                \"MnMaxT_C\" : TMxMin,\n",
    "                \"MnAveT_C\" : TAvMin,\n",
    "                \"MnMinT_C\" : TMnMin, }\n",
    "    zDenom = np.where( AllStateCnt > 0, np.array( AllStateCnt, dtype=np.float32), np.nan )\n",
    "    zMulti = 1.0 / zDenom\n",
    "    ZDDict = { \"All Counts\" : AllStateCnt,\n",
    "               \"ZMaxT_C\" : (allZMax * zMulti),\n",
    "               \"ZAveT_C\" : (allZAve * zMulti),\n",
    "               \"ZMinT_C\" : (allZMin * zMulti), }\n",
    "    # build our data frames\n",
    "    DaysIndexer = [ x for x in range(1, (TOTYRDAYS + 1), 1)]\n",
    "    ZAveDF = pd.DataFrame( index=DaysIndexer, data=ZDDict )\n",
    "    ZMaxDF = pd.DataFrame( index=DaysIndexer, data=ZMaxDict )\n",
    "    ZMinDF = pd.DataFrame( index=DaysIndexer, data=ZMinDict )\n",
    "    MaxDF = pd.DataFrame( index=DaysIndexer, data=MaxDict )\n",
    "    MinDF = pd.DataFrame( index=DaysIndexer, data=MinDict )\n",
    "    ZTAveDict[bKey] = ZAveDF\n",
    "    ZTMaxDict[bKey] = ZMaxDF\n",
    "    ZTMinDict[bKey] = ZMinDF\n",
    "    TrMaxDict[bKey] = MaxDF\n",
    "    TrMinDict[bKey] = MinDF\n",
    "    # now need to loop through again to calculate the matrix coefficients\n",
    "    # have average daily Z score for each variable. Need to go\n",
    "    #  back through and calculate correlation coefficients\n",
    "    r0_12_numer = 0.0\n",
    "    r0_12_1denom = 0.0\n",
    "    r0_12_2denom = 0.0\n",
    "    r1_11_numer = 0.0\n",
    "    r1_11_0denom = 0.0\n",
    "    r1_11_1denom = 0.0\n",
    "    r1_12_numer = 0.0\n",
    "    r1_12_1denom = 0.0\n",
    "    r1_12_2denom = 0.0\n",
    "    r1_21_numer = 0.0\n",
    "    r1_21_1denom = 0.0\n",
    "    r1_21_2denom = 0.0\n",
    "    r1_22_numer = 0.0\n",
    "    r1_22_0denom = 0.0\n",
    "    r1_22_1denom = 0.0    \n",
    "    for yY in range(StartYr, StopYr+1):\n",
    "        curStart = pd.Timestamp( yY, 1, 1, 0, )\n",
    "        curStop = pd.Timestamp( yY, 12, 31, 23, 59, )\n",
    "        curYrDF = cBasDF.loc[curStart:curStop].copy()\n",
    "        NumDays = len( curYrDF )\n",
    "        if NumDays > 365:\n",
    "            LastDay = TOTYRDAYS\n",
    "        else:\n",
    "            LastDay = ( TOTYRDAYS - 1 )\n",
    "        # end if\n",
    "        IsWet = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsDry = np.zeros( TOTYRDAYS, dtype=np.int32 )\n",
    "        IsWet[:LastDay] = np.where( curYrDF[\"Pre_mm\"] >= CutThresh, 1, 0 )\n",
    "        IsDry[:LastDay] = np.where( curYrDF[\"Pre_mm\"] < CutThresh, 1, 0 )\n",
    "        WStateCnt[:LastDay] += IsWet[:LastDay]\n",
    "        DStateCnt[:LastDay] += IsDry[:LastDay]\n",
    "        # now calculate Zs\n",
    "        # now calculate the Zs\n",
    "        curWAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDAve = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curWStd = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDStd = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        # max temperature\n",
    "        AveZ = ZAveDF.loc[:LastDay, \"ZMaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        ctNew = curYrDF[\"MaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        curWAve[:LastDay] = curWAves.loc[:LastDay, \"AMaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves.loc[:LastDay, \"AMaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        curWStd[:LastDay] = curWStds.loc[:LastDay, \"SDMaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        curDStd[:LastDay] = curDStds.loc[:LastDay, \"SDMaxT_C\"].to_numpy(dtype=np.float32)\n",
    "        curWStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curWStd, out=curWStdM, where=curWStd>0.01 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curDStd, out=curDStdM, where=curDStd>0.01 )\n",
    "        wZs = ( IsWet[:LastDay] * ( ( ctNew - curWAve[:LastDay] ) * curWStdM[:LastDay] ) )\n",
    "        dZs = ( IsDry[:LastDay] * ( ( ctNew - curDAve[:LastDay] ) * curDStdM[:LastDay] ) )\n",
    "        allZ = wZs + dZs\n",
    "        DiffZ1_0 = allZ - AveZ\n",
    "        mDiffZ1_0 = np.where( np.isnan( DiffZ1_0 ), 0.0, DiffZ1_0 )\n",
    "        DiffSqZ1_0 = (allZ - AveZ)**2.0\n",
    "        mDiffSqZ1_0 = np.where( np.isnan( DiffSqZ1_0 ), 0.0, DiffSqZ1_0 )\n",
    "        DiffZ1_1 = ( np.roll( allZ, 1 ) - AveZ )\n",
    "        mDiffZ1_1 = np.where( np.isnan( DiffZ1_1 ), 0.0, DiffZ1_1 )\n",
    "        # min temp\n",
    "        AveZ = ZAveDF.loc[:LastDay, \"ZMinT_C\"].to_numpy(dtype=np.float32)\n",
    "        ctNew = curYrDF[\"MinT_C\"].to_numpy(dtype=np.float32)\n",
    "        curWAve[:LastDay] = curWAves.loc[:LastDay, \"AMinT_C\"].to_numpy(dtype=np.float32)\n",
    "        curDAve[:LastDay] = curDAves.loc[:LastDay, \"AMinT_C\"].to_numpy(dtype=np.float32)\n",
    "        curWStd[:LastDay] = curWStds.loc[:LastDay, \"SDMinT_C\"].to_numpy(dtype=np.float32)\n",
    "        curDStd[:LastDay] = curDStds.loc[:LastDay, \"SDMinT_C\"].to_numpy(dtype=np.float32)\n",
    "        curWStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        curDStdM = np.zeros( TOTYRDAYS, dtype=np.float32 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curWStd, out=curWStdM, where=curWStd>0.01 )\n",
    "        np.divide( np.ones( TOTYRDAYS, dtype=np.float32 ), curDStd, out=curDStdM, where=curDStd>0.01 )\n",
    "        wZs = ( IsWet[:LastDay] * ( ( ctNew - curWAve[:LastDay] ) * curWStdM[:LastDay] ) )\n",
    "        dZs = ( IsDry[:LastDay] * ( ( ctNew - curDAve[:LastDay] ) * curDStdM[:LastDay] ) )\n",
    "        allZ = wZs + dZs\n",
    "        DiffZ2_0 = allZ - AveZ\n",
    "        mDiffZ2_0 = np.where( np.isnan( DiffZ2_0 ), 0.0, DiffZ2_0 )\n",
    "        DiffSqZ2_0 = (allZ - AveZ)**2.0\n",
    "        mDiffSqZ2_0 = np.where( np.isnan( DiffSqZ2_0 ), 0.0, DiffSqZ2_0 )\n",
    "        DiffZ2_1 = ( np.roll( allZ, 1 ) - AveZ )\n",
    "        mDiffZ2_1 = np.where( np.isnan( DiffZ2_1 ), 0.0, DiffZ2_1 )\n",
    "        # now do our running sums\n",
    "        r0_12_numer += np.dot( mDiffZ1_0, mDiffZ2_0 )\n",
    "        r0_12_1denom += mDiffSqZ1_0.sum()\n",
    "        r0_12_2denom += mDiffSqZ2_0.sum()\n",
    "        r1_11_numer += np.dot( mDiffZ1_0, mDiffZ1_1 )\n",
    "        r1_11_0denom += mDiffSqZ1_0.sum()\n",
    "        r1_11_1denom += mDiffSqZ1_0.sum()\n",
    "        r1_12_numer += np.dot( mDiffZ1_0, mDiffZ2_1 )\n",
    "        r1_12_1denom += mDiffSqZ1_0.sum()\n",
    "        r1_12_2denom += mDiffSqZ2_0.sum()\n",
    "        r1_21_numer += np.dot( mDiffZ2_0, mDiffZ1_1 )\n",
    "        r1_21_1denom += mDiffSqZ1_0.sum()\n",
    "        r1_21_2denom += mDiffSqZ2_0.sum()\n",
    "        r1_22_numer += np.dot( mDiffZ2_0, mDiffZ2_1 )\n",
    "        r1_22_0denom += mDiffSqZ2_0.sum()\n",
    "        r1_22_1denom += mDiffSqZ2_0.sum()\n",
    "    # end of year for\n",
    "    # calculate our coefficients\n",
    "    r0_12 = r0_12_numer / ( sqrt(r0_12_1denom) * sqrt(r0_12_2denom))\n",
    "    r1_11 = r1_11_numer / ( sqrt(r1_11_0denom) * sqrt(r1_11_1denom) )\n",
    "    r1_22 = r1_22_numer / ( sqrt(r1_22_0denom) * sqrt(r1_22_1denom) )\n",
    "    r1_12 = r1_12_numer / ( sqrt(r1_12_1denom) * sqrt(r1_12_2denom) )\n",
    "    r1_21 = r1_21_numer / ( sqrt(r1_21_2denom) * sqrt(r1_21_1denom) )\n",
    "    # now make our data frames\n",
    "    # rho 0 correlation\n",
    "    r0dpDDict = { \"rho_X1\" : [ 1.0, r0_12 ],\n",
    "                  \"rho_X2\" : [ r0_12, 1.0 ], }\n",
    "    r0Ind = [ \"rho_1X\", \"rho_2X\", ]\n",
    "    r0dpDF = pd.DataFrame( index=r0Ind, data=r0dpDDict )\n",
    "    # rho 1 cross-correlation\n",
    "    r1dpDDict = { \"rho_X1_L1\" : [ r1_11, r1_12,],\n",
    "                  \"rho_X2_L1\" : [ r1_21, r1_22,], }\n",
    "    r1dpDF = pd.DataFrame( index=r0Ind, data=r1dpDDict )\n",
    "    # assign to basin tracking dictionaries\n",
    "    ZTRho0Dict[bKey] = r0dpDF\n",
    "    ZTRho1Dict[bKey] = r1dpDF\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf25d50-3303-425a-a970-97b6a2495354",
   "metadata": {},
   "source": [
    "Serialize the tracking dictionaries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc7f0a1a-8d11-44d5-bd19-55b2b546b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_ZAve_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( ZTAveDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb8679af-faa7-497c-991e-03eb5276278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_ZMax_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( ZTMaxDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8921b555-14fe-4e36-b2fb-3457c2be2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_ZMin_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( ZTMinDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a682f813-5a42-4ccf-adf3-605cf5098689",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_TempMax_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( TrMaxDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fa916db-20d1-4605-a96d-1077798c0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_TempMin_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( TrMinDict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b359bd1-9dbf-4f1b-9e94-30a7b52e99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_Rho0_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( ZTRho0Dict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a8cfcaf-7676-480b-9b30-33036b770d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutFiler = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_Rho1_%s-%s_DFDict.pkl\" % (StartYr, StopYr) ) )\n",
    "with open( OutFiler, 'wb' ) as OF:\n",
    "    pickle.dump( ZTRho1Dict, OF, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "# end with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3813ea-2948-4e8a-b169-608735acb20b",
   "metadata": {},
   "source": [
    "Output the correlation DataFrames to spreadsheets for easy examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "421bf54e-e1c0-43eb-a40a-4ba2fc91527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outXLSX = os.path.normpath( os.path.join( OUT_DIR, \"OWeath_Rho0and1_byBasin.xlsx\" ) )\n",
    "writer = pd.ExcelWriter( outXLSX )\n",
    "workbook  = writer.book\n",
    "format1 = workbook.add_format({'num_format': '#,##0.000000'})\n",
    "for bKey in basKeys:\n",
    "    curDF = ZTRho0Dict[bKey]\n",
    "    cLabel = \"%s_rho0\" % bKey\n",
    "    curDF.to_excel( writer, sheet_name=cLabel, )\n",
    "    # adjust columns\n",
    "    writer.sheets[cLabel].set_column( 0, 0, 15 )\n",
    "    for column in curDF:\n",
    "        column_width = max(curDF[column].astype(str).map(len).max()+6, len(column)+6)\n",
    "        col_idx = curDF.columns.get_loc(column)\n",
    "        writer.sheets[cLabel].set_column(col_idx+1, col_idx+1, column_width, format1)\n",
    "    # end for\n",
    "    curDF = ZTRho1Dict[bKey]\n",
    "    cLabel = \"%s_rho1\" % bKey\n",
    "    curDF.to_excel( writer, sheet_name=cLabel, )\n",
    "    # adjust columns\n",
    "    writer.sheets[cLabel].set_column( 0, 0, 15 )\n",
    "    for column in curDF:\n",
    "        column_width = max(curDF[column].astype(str).map(len).max()+6, len(column)+6)\n",
    "        col_idx = curDF.columns.get_loc(column)\n",
    "        writer.sheets[cLabel].set_column(col_idx+1, col_idx+1, column_width, format1)\n",
    "    # end for\n",
    "# end for\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
